{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2019-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# Code to generate phrase representations from a pre-trained model, according to their positions.\n",
    "# This can be used to initialize a cross-lingual classifier, for instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "# this notebook should be under XLM-master to use the classes in src/ (downloaded from github)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from src.utils import AttrDict\n",
    "from src.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from src.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n"
     ]
    }
   ],
   "source": [
    "# first, manually download corresponding models \n",
    "# pretraining: MLM + TLM, preprocessing: tokenize + lowercase + no accent + BPE \n",
    "model_path = 'models/mlm_tlm_xnli15_1024.pth'       \n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)  # 0\n",
    "params.eos_index = dico.index(EOS_WORD)  # 1 \n",
    "params.pad_index = dico.index(PAD_WORD)  # 2 \n",
    "params.unk_index = dico.index(UNK_WORD)  # 3 \n",
    "params.mask_index = dico.index(MASK_WORD) # 5 \n",
    "\n",
    "print(params.n_words)\n",
    "\n",
    "# build model / reload weights\n",
    "# TransformerModel : def __init__(self, params, dico, is_encoder, with_output):\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "model.load_state_dict(reloaded['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get phrase/word representations \n",
    "\n",
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE.\n",
    "\n",
    "Trace splitted token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = \"./models/codes_xnli_15.txt\"    \n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "# bpe-ize a list of sentences \n",
    "def to_bpe(sentences):\n",
    "    with open('./tmp/sentences', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    # applybpe output input codes [vocab]  => apply BPE codes to a text file\n",
    "    os.system('%s applybpe ./tmp/sentences.bpe ./tmp/sentences %s' % (fastbpe, codes))\n",
    "    sentences_bpe = []\n",
    "    with open('./tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())  \n",
    "    return sentences_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentences = [\n",
    "#    'once he had worn trendy italian leather shoes and jeans from paris that had cost three hundred euros .', # en\n",
    "#    'Le français est la seule langue étrangère proposée dans le système éducatif .', # fr\n",
    "#    'El cadmio produce efectos tóxicos en los organismos vivos , aun en concentraciones muy pequeñas .', # es\n",
    "#    'Nach dem Zweiten Weltkrieg verbreitete sich Bonsai als Hobby in der ganzen Welt .', # de\n",
    "#    'وقد فاز في الانتخابات في الجولة الثانية من التصويت من قبل سيدي ولد الشيخ عبد الله ، مع أحمد ولد داداه في المرتبة الثانية .', # ar\n",
    "#    '羅伯特 · 皮爾 斯 生於 1863年 , 在 英國 曼徹斯特 學習 而 成為 一 位 工程師 . 1933年 , 皮爾斯 在 直布羅陀去世 .', # zh\n",
    "#]\n",
    "#sentences = to_bpe(sentences)\n",
    "#print(sentences)\n",
    "#print('\\n\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of out-of-vocab words: 0/67\n",
      "[['</s>', 'for', 'some', 'time', 'i', 'have', 'been', 'interested', 'in', 'the', 'pl@@', 'ace@@', 'bo', 'effect', ',', 'which', 'might', 'seem', 'like', 'an', 'odd', 'thing', 'for', 'a', 'mag@@', 'ic@@', 'ian', 'to', 'be', 'interested', 'in', ',', 'unless', 'you', 'think', 'of', 'it', 'in', 'the', 'ter@@', 'ms', 'that', 'i', 'do', ',', 'which', 'is', ',', '`@@', '`', 'something', 'f@@', 'ake', 'is', 'believed', 'in', 'enough', 'by', 'somebody', 'that', 'it', 'be@@', 'comes', 'something', 'real', '.', \"'@@\", \"'\", '</s>']]\n"
     ]
    }
   ],
   "source": [
    "uncased = \"for some time i have been interested in the placebo effect , which might seem like an odd thing for a magician to be interested in , unless you think of it in the terms that i do , which is , `` something fake is believed in enough by somebody that it becomes something real . ''\"\n",
    "bpe = \"for some time i have been interested in the pl@@ ace@@ bo effect , which might seem like an odd thing for a mag@@ ic@@ ian to be interested in , unless you think of it in the ter@@ ms that i do , which is , `@@ ` something f@@ ake is believed in enough by somebody that it be@@ comes something real . '@@ '\"\n",
    "bpe1 = \"in other words , sugar pil@@ ls have a me@@ as@@ ur@@ able effect in certain kin@@ ds of studies , the pl@@ ace@@ bo effect , just because the person thinks that what 's happening to them is a pharmac@@ eut@@ ical or some sort of a -- for pain management , for ex@@ ample , if they believe it enough there is a me@@ as@@ ur@@ able effect in the body called the pl@@ ace@@ bo effect .\"\n",
    "\n",
    "sentences = []\n",
    "sentences.append(bpe)\n",
    "#sentences.append(bpe1)\n",
    "#print(sentences)\n",
    "\n",
    "# check how many tokens are OOV\n",
    "n_w = len([w for w in ' '.join(sentences).split()])   # count all words in list of sentences \n",
    "n_oov = len([w for w in ' '.join(sentences).split() if w not in dico.word2id])\n",
    "print('Number of out-of-vocab words: %s/%s' % (n_oov, n_w))\n",
    "\n",
    "# add </s> sentence delimiters. form a list of lists  \n",
    "# each sentence becomes a list of bpe tokens, with </s> at the beginning and the end  \n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split()) for sent in sentences]\n",
    "print(sentences)\n",
    "#print(len(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, [10, 11, 12], 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, [24, 25, 26], 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, [39, 40], 41, 42, 43, 44, 45, 46, 47, [48, 49], 50, [51, 52], 53, 54, 55, 56, 57, 58, 59, 60, [61, 62], 63, 64, 65, [66, 67], 68]\n",
      "[10, 11, 12]\n",
      "pl@@ ace@@ bo\n"
     ]
    }
   ],
   "source": [
    "# update indices after doing this: '</s> sent </s>' \n",
    "bpe_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, [9, 10, 11], 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, [23, 24, 25], 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, [38, 39], 40, 41, 42, 43, 44, 45, 46, [47, 48], 49, [50, 51], 52, 53, 54, 55, 56, 57, 58, 59, [60, 61], 62, 63, 64, [65, 66]]\n",
    "\n",
    "new = []\n",
    "for elem in bpe_indices:\n",
    "    if isinstance(elem, int):\n",
    "        new.append(elem+1)\n",
    "    elif isinstance(elem, list):\n",
    "        new.append([x+1 for x in elem])\n",
    "#print(new)\n",
    "\n",
    "last_element = new[-1]\n",
    "if isinstance(last_element, int):\n",
    "    final = [0] + new + [last_element+1]\n",
    "elif isinstance(last_element, list):\n",
    "    final = [0] + new + [last_element[-1]+1]\n",
    "\n",
    "print(final)\n",
    "# word alignment in tedannnote should also be updated! +1 \n",
    "\n",
    "index = 10\n",
    "bpe_index = final[index] \n",
    "\n",
    "print(bpe_index)\n",
    "\n",
    "sent = sentences[0] \n",
    "if isinstance(bpe_index, int):\n",
    "    print(sent[bpe_index])\n",
    "elif isinstance(bpe_index, list):\n",
    "    print(' '.join([sent[i] for i in bpe_index]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "#print([len(sent) for sent in sentences])\n",
    "slen = max([len(sent) for sent in sentences])\n",
    "#print(slen)\n",
    "\n",
    "#print(torch.LongTensor(slen, bs))  # random values in the matrix \n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "#print(word_ids)\n",
    "#word_ids.shape: 69 rows (max sent length), _ columns (number of sentences)\n",
    "\n",
    "for i in range(len(sentences)):   # for each sentence \n",
    "    #print([w for w in sentences[i]])\n",
    "    #print([dico.index(w) for w in sentences[i]])\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i]])\n",
    "    #print(sent)\n",
    "    # i: column indice, refers to each sentence \n",
    "    # fill the matrix column by column \n",
    "    # take the first len(sent) rows of cells in the current column. the others remain padded \n",
    "    word_ids[:len(sent), i] = sent\n",
    "#print(word_ids)\n",
    "lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "#print(lengths)\n",
    "\n",
    "# NOTE: No more language id (removed it in a later version)\n",
    "# langs = torch.LongTensor([params.lang2id[lang] for _, lang in sentences]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None\n",
    "langs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12]\n",
      "tensor([[ -4.2908,  -0.0421,  -1.9604,  ...,   2.3874,  -1.3416, -13.1386]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 2.1268,  2.2024,  0.5680,  ...,  0.5634, -0.5559, -4.5375]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[-0.4283, -1.1916,  1.2353,  ...,  0.7101, -2.6170, -9.7668]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[[ -4.2908,  -0.0421,  -1.9604,  ...,   2.3874,  -1.3416, -13.1386]],\n",
      "\n",
      "        [[  2.1268,   2.2024,   0.5680,  ...,   0.5634,  -0.5559,  -4.5375]],\n",
      "\n",
      "        [[ -0.4283,  -1.1916,   1.2353,  ...,   0.7101,  -2.6170,  -9.7668]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "tensor([[ 2.1268,  2.2024,  1.2353,  ...,  2.3874, -0.5559, -4.5375]],\n",
      "       grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "# 69: max sentence length\n",
    "# 1: number of sentences, i.e. batch size \n",
    "# 1024 hidden states of the model: each word representation has 1024 dimensions \n",
    "# 1个大tensor内包含69个matrix, 每个matrix 1 row, 1024 columns (dimensions) \n",
    "#print(tensor.size())   # torch.Size([69, 1, 1024])\n",
    "#print(tensor)\n",
    "\n",
    "print(bpe_index)\n",
    "\n",
    "a = []\n",
    "if isinstance(bpe_index, list):\n",
    "    for x in bpe_index:\n",
    "        print(tensor[x]) \n",
    "        a.append(tensor[x])\n",
    "print(torch.stack(a))     # turn a list of tensors to one tensor \n",
    "\n",
    "# max-pooling of input tensors \n",
    "print(torch.max(torch.stack(a), 0).values)\n",
    "\n",
    "#### until here, I managed to get representation of one source phrase after max-pooling \n",
    "\n",
    "#print(tensor[0][0].size())  # first hidden state of the first sentence \n",
    "#print(tensor[1].size())  # second hidden state of all the sentences in the batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable tensor is of shape (max_sequence_length, batch_size, model_dimension).\n",
    "\n",
    "tensor[0] is a tensor of shape (batch_size, model_dimension) that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is the vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_id=dico.index('cat')\n",
    "#print(word_id)\n",
    "#emb = model.embeddings.weight[word_id]\n",
    "#print(emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tensor = model.embeddings(word_ids)\n",
    "#print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
