{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2019-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# Code to generate sentence representations from a pre-trained model.\n",
    "# This can be used to initialize a cross-lingual classifier, for instance.\n",
    "\n",
    "# Attention, this notebook contains the same content as generate-embeddings.ipynb \n",
    "# but I write many comments here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "# this notebook should be under XLM-master to use the classes in src/ (downloaded from github)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from src.utils import AttrDict\n",
    "from src.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from src.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n"
     ]
    }
   ],
   "source": [
    "# first, manually download corresponding models \n",
    "# pretraining: MLM + TLM, preprocessing: tokenize + lowercase + no accent + BPE \n",
    "model_path = 'models/mlm_tlm_xnli15_1024.pth'       \n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)\n",
    "\n",
    "print(params.n_words)\n",
    "\n",
    "# build model / reload weights\n",
    "# TransformerModel : def __init__(self, params, dico, is_encoder, with_output):\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "# I don't understand here:  \n",
    "model.eval()\n",
    "model.load_state_dict(reloaded['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentence representations\n",
    "\n",
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE.\n",
    "\n",
    "Below you can see an example for English, French, Spanish, German, Arabic and Chinese sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = \"./models/codes_xnli_15.txt\"    # path to the codes of the model  \n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    with open('./tmp/sentences', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    # applybpe output input codes [vocab]  => apply BPE codes to a text file\n",
    "    os.system('%s applybpe ./tmp/sentences.bpe ./tmp/sentences %s' % (fastbpe, codes))\n",
    "    sentences_bpe = []\n",
    "    with open('./tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())  \n",
    "    return sentences_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once he had worn tren@@ dy itali@@ an leather shoes and jeans from paris that had cost three hundred euros .\n",
      "\n",
      "L@@ e fran@@ ç@@ ais est la seule langue é@@ tr@@ ang@@ è@@ re pro@@ pos@@ é@@ e dans le syst@@ è@@ me é@@ du@@ cat@@ if .\n",
      "\n",
      "E@@ l cad@@ mio produce efectos t@@ ó@@ x@@ icos en los organismos viv@@ os , aun en concentr@@ aciones muy pe@@ que@@ ñ@@ as .\n",
      "\n",
      "N@@ ach dem Z@@ weiten W@@ el@@ t@@ krieg verbreit@@ ete sich B@@ on@@ sai als H@@ ob@@ by in der ganzen W@@ elt .\n",
      "\n",
      "وقد فاز في الانتخابات في الج@@ ولة الثانية من التصويت من قبل سيدي ولد الشيخ عبد الله ، مع أ@@ حمد ولد د@@ اد@@ اه في المرتبة الثانية .\n",
      "\n",
      "羅@@ 伯特 · 皮@@ 爾 斯 生於 186@@ 3年 , 在 英國 曼@@ 徹@@ 斯特 學習 而 成為 一 位 工程@@ 師 . 1933年 , 皮@@ 爾@@ 斯 在 直@@ 布@@ 羅@@ 陀@@ 去世 .\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'once he had worn trendy italian leather shoes and jeans from paris that had cost three hundred euros .', # en\n",
    "    'Le français est la seule langue étrangère proposée dans le système éducatif .', # fr\n",
    "    'El cadmio produce efectos tóxicos en los organismos vivos , aun en concentraciones muy pequeñas .', # es\n",
    "    'Nach dem Zweiten Weltkrieg verbreitete sich Bonsai als Hobby in der ganzen Welt .', # de\n",
    "    'وقد فاز في الانتخابات في الجولة الثانية من التصويت من قبل سيدي ولد الشيخ عبد الله ، مع أحمد ولد داداه في المرتبة الثانية .', # ar\n",
    "    '羅伯特 · 皮爾 斯 生於 1863年 , 在 英國 曼徹斯特 學習 而 成為 一 位 工程師 . 1933年 , 皮爾斯 在 直布羅陀去世 .', # zh\n",
    "]\n",
    "sentences = to_bpe(sentences)\n",
    "print('\\n\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of out-of-vocab words: 17/164\n",
      "['</s>', 'once', 'he', 'had', 'worn', 'tren@@', 'dy', 'itali@@', 'an', 'leather', 'shoes', 'and', 'jeans', 'from', 'paris', 'that', 'had', 'cost', 'three', 'hundred', 'euros', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# check how many tokens are OOV\n",
    "n_w = len([w for w in ' '.join(sentences).split()])\n",
    "n_oov = len([w for w in ' '.join(sentences).split() if w not in dico.word2id])\n",
    "print('Number of out-of-vocab words: %s/%s' % (n_oov, n_w))\n",
    "# add </s> sentence delimiters. form a list of lists \n",
    "# don't foreget here, we append </s> \n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split()) for sent in sentences]\n",
    "b = sentences \n",
    "#print(sentences)\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "#print([len(sent) for sent in sentences])\n",
    "slen = max([len(sent) for sent in sentences])\n",
    "\n",
    "#print(dico.id2word[5])\n",
    "#params.bos_index 0 <s> \n",
    "#params.eos_index 1 </s>  but </s> is also used for bos  \n",
    "#params.pad_index 2 <pad>\n",
    "#params.unk_index 3 <unk>       4: <special0> \n",
    "#params.mask_index 5 <special1> \n",
    "\n",
    "#print(torch.LongTensor(slen, bs))  # random values in the matrix \n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "#print(word_ids)\n",
    "# word_ids.shape: 37 rows (max sent length), 6 columns (number of sentences)\n",
    "\n",
    "for i in range(len(sentences)):  # i from 0 to 5 \n",
    "    #print([w for w in sentences[i]])\n",
    "    #print([dico.index(w) for w in sentences[i]])\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i]])\n",
    "    #print(sent)\n",
    "    # i: column indice, refers to each sentence \n",
    "    # fill the matrix column by column \n",
    "    # take the first len(sent) rows of cells in the current column. the others remain padded \n",
    "    word_ids[:len(sent), i] = sent\n",
    "#print(word_ids)\n",
    "lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "#print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: No more language id (removed it in a later version)\n",
    "# langs = torch.LongTensor([params.lang2id[lang] for _, lang in sentences]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None\n",
    "langs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 6, 1024])\n",
      "tensor([[-0.4258,  3.7078, -1.2446,  ...,  1.6236,  1.8969, -7.4575],\n",
      "        [-1.4627,  5.1548, -4.7037,  ..., -0.6723,  3.7204,  0.4129],\n",
      "        [-0.4615,  8.3762, -3.5408,  ..., -0.7099,  3.9102, -2.8770],\n",
      "        [-1.0137,  2.1509, -1.8360,  ..., -1.2961,  2.6104,  2.4603],\n",
      "        [ 2.5150,  2.6423, -1.1369,  ..., -0.3289,  3.2460, -7.8119],\n",
      "        [ 1.1674, -0.5507, -2.0623,  ..., -0.8173,  2.9915,  1.9788]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([1024])\n",
      "torch.Size([6, 1024])\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "# 1024 hidden states of the model: each word embedding has 1024 dimensions \n",
    "# 37: max sentence length\n",
    "# 6: number of sentences, i.e. batch size \n",
    "# 1个大matrix内包含37个2维matrix, 每个6rows, 1024 columns (dimensions) \n",
    "print(tensor.size())\n",
    "print(tensor[0])\n",
    "\n",
    "print(tensor[0][0].size())  # first hidden state of the first sentence \n",
    "print(tensor[1].size())  # second hidden state of all the sentences in the batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable tensor is of shape (max_sequence_length, batch_size, model_dimension).\n",
    "\n",
    "tensor[0] is a tensor of shape (batch_size, model_dimension) that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is the vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9671\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "word_id=dico.index('cat')\n",
    "print(word_id)\n",
    "emb = model.embeddings.weight[word_id]\n",
    "print(emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 6, 1024])\n"
     ]
    }
   ],
   "source": [
    "tensor = model.embeddings(word_ids)\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
